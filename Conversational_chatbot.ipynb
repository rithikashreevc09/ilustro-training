{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string  # to process standard python strings\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Downloading necessary nltk packages (first-time only)\n",
        "nltk.download('punkt')  # for tokenization\n",
        "nltk.download('wordnet')  # for lemmatization\n",
        "\n",
        "# Example corpus for chatbot (FAQs)\n",
        "faq_corpus = [\n",
        "    \"Hello, how can I help you?\",\n",
        "    \"Hi there, how can I assist?\",\n",
        "    \"What is natural language processing?\",\n",
        "    \"NLP is a field of AI that gives machines the ability to read, understand, and derive meaning from human language.\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Machine learning is the study of computer algorithms that improve automatically through experience.\",\n",
        "    \"How does deep learning work?\",\n",
        "    \"Deep learning is a subset of machine learning that uses neural networks with many layers (deep architectures).\",\n",
        "    \"Who created you?\",\n",
        "    \"I was created by a team of AI enthusiasts.\",\n",
        "    \"Thank you!\",\n",
        "    \"You're welcome!\",\n",
        "    \"Goodbye!\"\n",
        "]\n",
        "\n",
        "faq_responses = [\n",
        "    \"Hello! How can I assist you today?\",\n",
        "    \"Hi! What can I do for you?\",\n",
        "    \"Natural language processing, or NLP, is a branch of AI that deals with the interaction between computers and humans through language.\",\n",
        "    \"NLP involves enabling computers to understand and respond to human language, helping in tasks such as translation, summarization, and conversation.\",\n",
        "    \"Machine learning refers to algorithms that improve based on experience or data.\",\n",
        "    \"Machine learning is about creating models that allow computers to learn patterns from data and make decisions or predictions.\",\n",
        "    \"Deep learning involves neural networks with multiple layers, often used for complex tasks such as image recognition, language translation, and more.\",\n",
        "    \"I was created using Python, machine learning, and some good old-fashioned programming.\",\n",
        "    \"I'm glad I could help! Anything else?\",\n",
        "    \"You're very welcome!\",\n",
        "    \"Goodbye! Feel free to come back if you need more help!\"\n",
        "]\n",
        "\n",
        "# Tokenization\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text.lower())\n",
        "\n",
        "# Lemmatization\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "# Greeting responses\n",
        "greeting_inputs = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
        "greeting_responses = [\"Hello\", \"Hi\", \"Greetings!\", \"Hey there!\", \"Hello! How can I assist you today?\"]\n",
        "\n",
        "def greeting(sentence):\n",
        "    \"\"\"Return a greeting response if the user's input is a greeting.\"\"\"\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in greeting_inputs:\n",
        "            return random.choice(greeting_responses)\n",
        "\n",
        "# Generate response based on similarity\n",
        "def generate_response(user_input):\n",
        "    bot_response = ''\n",
        "    faq_corpus.append(user_input)  # Add user input to the corpus for similarity checking\n",
        "    tfidf_vectorizer = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(faq_corpus)\n",
        "\n",
        "    # Compute similarity between user input and corpus\n",
        "    similarity_values = cosine_similarity(tfidf_matrix[-1], tfidf_matrix)\n",
        "    index = similarity_values.argsort()[0][-2]  # Get the most similar response\n",
        "    flat = similarity_values.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "\n",
        "    if req_tfidf == 0:\n",
        "        bot_response = \"I am sorry, I don't understand. Could you rephrase that?\"\n",
        "    else:\n",
        "        bot_response = faq_responses[index]\n",
        "\n",
        "    faq_corpus.pop()  # Remove user input from the corpus\n",
        "    return bot_response\n",
        "\n",
        "# Main chatbot interaction loop\n",
        "def chatbot():\n",
        "    print(\"Chatbot: Hello! I am an NLP chatbot. Type 'bye' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").lower()\n",
        "        if user_input == 'bye':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "        else:\n",
        "            if greeting(user_input) is not None:\n",
        "                print(f\"Chatbot: {greeting(user_input)}\")\n",
        "            else:\n",
        "                print(f\"Chatbot: {generate_response(user_input)}\")\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sebhDNNe3Qra",
        "outputId": "fbb6961c-dd3c-4b15-e97a-fd0acdddea2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Hello! I am an NLP chatbot. Type 'bye' to exit.\n",
            "You: hii\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I am sorry, I don't understand. Could you rephrase that?\n",
            "You: hi\n",
            "Chatbot: Hey there!\n",
            "You: who created you?\n",
            "Chatbot: I'm glad I could help! Anything else?\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    }
  ]
}